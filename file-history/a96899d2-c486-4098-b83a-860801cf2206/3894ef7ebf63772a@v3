"""
æ•°æ®åŠ è½½æ¨¡å—

é›†ä¸­ç®¡ç†æ‰€æœ‰æ•°æ®è¯»å–é€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
- äº§å“å‡€å€¼æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰
- äº§å“è¿ä½œæ¦‚è§ˆæ•°æ®
- æ¸ é“ä¿æœ‰é‡æ•°æ®
- åº•å±‚æ•°æ®æ±‡æ€»
- å‘¨åº¦æ›´æ–°æ•°æ®
- å„ç±»Excelæ•°æ®æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    from load_data import DataLoader

    loader = DataLoader()
    nav_data, nav_pivot = loader.load_nav_data()
    yunzuogailan = loader.load_yunzuogailan(date)
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pickle
import glob
import warnings
from typing import Tuple, Dict, Optional, List

from config import Config

warnings.filterwarnings("ignore")


class DataLoader:
    """
    ç»Ÿä¸€æ•°æ®åŠ è½½å™¨

    é›†ä¸­ç®¡ç†æ‰€æœ‰æ•°æ®è¯»å–æ“ä½œï¼Œæ”¯æŒç¼“å­˜å’Œæ•°æ®éªŒè¯
    """

    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        self._cache = {}  # å†…å­˜ç¼“å­˜
        self._product_info_df = None  # DAPäº§å“ä¿¡æ¯è¡¨ç¼“å­˜

    # ==================== 1. äº§å“å‡€å€¼æ•°æ® ====================

    def load_nav_data(self, force_rebuild: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        è¯»å–äº§å“å‡€å€¼æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶å¤§å¹…æå‡æ€§èƒ½

        ç¼“å­˜ç­–ç•¥ï¼š
        1. é¦–æ¬¡è¿è¡Œï¼šè¯»å–æ‰€æœ‰Excelæ–‡ä»¶ï¼Œä¿å­˜ä¸ºparquetç¼“å­˜
        2. åç»­è¿è¡Œï¼šåªè¯»å–æ–°å¢/ä¿®æ”¹çš„æ–‡ä»¶ï¼Œä¸ç¼“å­˜åˆå¹¶
        3. ç›¸åŒæ—¥æœŸ+äº§å“ä»£ç çš„æ•°æ®ï¼Œä¿ç•™æœ€æ–°æ–‡ä»¶çš„å€¼

        å‚æ•°ï¼š
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç¼“å­˜ï¼ˆé»˜è®¤Falseï¼‰

        è¿”å›ï¼š
            Tuple[DataFrame, DataFrame]: (åŸå§‹æ•°æ®, æ•°æ®é€è§†è¡¨)
        """
        import time

        COOKED_FOLDER = Config.NET_VALUE_DIR

        # ç¼“å­˜ç›®å½•å’Œæ–‡ä»¶
        cache_dir = os.path.join(os.path.dirname(COOKED_FOLDER), '.nav_cache')
        cache_data_file = os.path.join(cache_dir, 'nav_master_data.parquet')
        cache_meta_file = os.path.join(cache_dir, 'nav_metadata.pkl')

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)

        # è·å–æ‰€æœ‰Excelæ–‡ä»¶åŠå…¶ä¿®æ”¹æ—¶é—´
        excel_files = glob.glob(os.path.join(COOKED_FOLDER, '*.xlsx'))
        excel_files = [f for f in excel_files if not os.path.basename(f).startswith('~$')]

        current_files_info = {}
        for f in excel_files:
            mtime = os.path.getmtime(f)
            current_files_info[f] = mtime

        print(f"ğŸ“‚ å‡€å€¼æ•°æ®æ–‡ä»¶å¤¹: {COOKED_FOLDER}")
        print(f"ğŸ“Š å‘ç° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")

        # åŠ è½½ç¼“å­˜å…ƒæ•°æ®
        cached_files_info = {}
        cached_data = None

        if not force_rebuild and os.path.exists(cache_meta_file) and os.path.exists(cache_data_file):
            try:
                with open(cache_meta_file, 'rb') as f:
                    cached_files_info = pickle.load(f)
                cached_data = pd.read_parquet(cache_data_file)
                print(f"âœ… å·²åŠ è½½ç¼“å­˜æ•°æ®: {len(cached_data)} æ¡è®°å½•")
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡å»ºç¼“å­˜: {e}")
                cached_files_info = {}
                cached_data = None

        # æ‰¾å‡ºéœ€è¦è¯»å–çš„æ–‡ä»¶ï¼ˆæ–°å¢æˆ–ä¿®æ”¹çš„ï¼‰
        files_to_read = []
        for f, mtime in current_files_info.items():
            if f not in cached_files_info or cached_files_info[f] != mtime:
                files_to_read.append(f)

        needed_cols = ['ä¼°å€¼æ—¥', 'äº§å“ä»£ç ', 'ç´¯è®¡å‡€å€¼']

        if len(files_to_read) == 0 and cached_data is not None:
            print(f"âœ… æ‰€æœ‰æ–‡ä»¶å‡å·²ç¼“å­˜ï¼Œæ— éœ€é‡æ–°è¯»å–")
            data = cached_data
        else:
            print(f"ğŸ“– éœ€è¦è¯»å– {len(files_to_read)} ä¸ªæ–‡ä»¶ï¼ˆæ–°å¢æˆ–å·²ä¿®æ”¹ï¼‰")

            # è¯»å–æ–°æ–‡ä»¶
            start_time = time.time()
            new_dfs = []

            for i, f in enumerate(files_to_read, 1):
                if i % 10 == 0 or i == len(files_to_read):
                    print(f"   è¯»å–è¿›åº¦: {i}/{len(files_to_read)} - {os.path.basename(f)}")
                try:
                    df = pd.read_excel(f, usecols=needed_cols)
                    df['_source_file'] = f
                    new_dfs.append(df)
                except Exception as e:
                    print(f"   âš ï¸ è¯»å–å¤±è´¥: {os.path.basename(f)} - {e}")

            read_time = time.time() - start_time
            print(f"   è¯»å–è€—æ—¶: {read_time:.1f} ç§’")

            # åˆå¹¶æ–°æ•°æ®ä¸ç¼“å­˜æ•°æ®
            if new_dfs:
                new_data = pd.concat(new_dfs, ignore_index=True)

                if cached_data is not None and len(files_to_read) < len(excel_files):
                    modified_files = [f for f in files_to_read if f in cached_files_info]
                    if modified_files and '_source_file' in cached_data.columns:
                        cached_data = cached_data[~cached_data['_source_file'].isin(modified_files)]

                    data = pd.concat([cached_data, new_data], ignore_index=True)
                    print(f"   å¢é‡åˆå¹¶å®Œæˆ: ç¼“å­˜ {len(cached_data)} + æ–°å¢ {len(new_data)} æ¡")
                else:
                    data = new_data
            else:
                data = cached_data if cached_data is not None else pd.DataFrame(columns=needed_cols)

        # å»é‡
        before_dedup = len(data)
        data.drop_duplicates(subset=['ä¼°å€¼æ—¥', 'äº§å“ä»£ç '], keep='last', inplace=True)
        print(f"   å»é‡: {before_dedup} -> {len(data)} æ¡è®°å½•")

        # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
        if len(files_to_read) > 0 or not os.path.exists(cache_data_file):
            try:
                if 'äº§å“ä»£ç ' in data.columns:
                    data['äº§å“ä»£ç '] = data['äº§å“ä»£ç '].astype(str)
                data.to_parquet(cache_data_file, index=False)
                with open(cache_meta_file, 'wb') as f:
                    pickle.dump(current_files_info, f)
                print(f"ğŸ’¾ ç¼“å­˜å·²æ›´æ–°: {cache_data_file}")
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        # æ„å»ºé€è§†è¡¨
        print(f"ğŸ”„ æ„å»ºæ•°æ®é€è§†è¡¨...")
        pivot_data = data[['ä¼°å€¼æ—¥', 'äº§å“ä»£ç ', 'ç´¯è®¡å‡€å€¼']].copy()

        data_transposition = pd.pivot_table(
            pivot_data,
            index='ä¼°å€¼æ—¥',
            columns='äº§å“ä»£ç ',
            values='ç´¯è®¡å‡€å€¼',
            aggfunc='median'
        )

        try:
            data_transposition.index = pd.to_datetime(data_transposition.index, format='%Y/%m/%d')
        except:
            data_transposition.index = pd.to_datetime(data_transposition.index, errors='coerce')

        data_transposition = data_transposition.sort_index()
        data_transposition.columns = data_transposition.columns.astype(str)

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(data_transposition)} ä¸ªäº¤æ˜“æ—¥, {len(data_transposition.columns)} ä¸ªäº§å“")

        return data, data_transposition

    # ==================== 2. DAPäº§å“ä¿¡æ¯è¡¨ ====================

    def load_product_info_dap(self) -> pd.DataFrame:
        """
        åŠ è½½DAPäº§å“ä¿¡æ¯è¡¨

        è¿”å›ï¼š
            DataFrame: äº§å“ä¿¡æ¯è¡¨ï¼Œä»¥äº§å“ä»£ç ä¸ºç´¢å¼•
        """
        if self._product_info_df is None:
            self._product_info_df = pd.read_excel(
                Config.get_weekly_data_file('ç†è´¢äº§å“ä¿¡æ¯æŸ¥è¯¢'),
                index_col=0
            )
        return self._product_info_df

    # ==================== 3. äº§å“è¿ä½œæ¦‚è§ˆæ•°æ® ====================

    def load_yunzuogailan_raw(self) -> pd.DataFrame:
        """
        åŠ è½½åŸå§‹äº§å“è¿ä½œæ¦‚è§ˆæ•°æ®

        è¿”å›ï¼š
            DataFrame: åŸå§‹è¿ä½œæ¦‚è§ˆæ•°æ®
        """
        yunzuogailan_before = pd.read_excel(
            Config.get_weekly_data_file('äº§å“è¿ä½œæ¦‚è§ˆ'),
            index_col=0
        )
        yunzuogailan_before.drop_duplicates(inplace=True)
        # å¤„ç†paså¯¼å‡ºçš„ä¸šç»©åŸºå‡†ï¼Œxå·è¢«å¯¼å‡ºä¸º&times;
        yunzuogailan_before['ä¸šç»©åŸºå‡†'] = yunzuogailan_before['ä¸šç»©åŸºå‡†'].map(
            lambda s: s.replace("&times;", "*") if (isinstance(s, str)) and ("&times;" in s) else s
        )
        return yunzuogailan_before

    def load_baoyou_data(self) -> pd.DataFrame:
        """
        åŠ è½½å„æ¸ é“ä¿æœ‰é‡æ•°æ®

        è¿”å›ï¼š
            DataFrame: æ¸ é“ä¿æœ‰é‡æ•°æ®
        """
        Baoyou_df = pd.read_excel(
            Config.get_weekly_data_file('å„æ¸ é“ä¿æœ‰é‡'),
            index_col=0
        )
        # åˆ é™¤æ¸ é“ä¿æœ‰é‡ä¸­å­äº§å“çš„æ¸ é“ä¸º-çš„
        Baoyou_df_sub = Baoyou_df[Baoyou_df['æ¸ é“åç§°'] != '-']
        Baoyou_df_mom = Baoyou_df[Baoyou_df.apply(lambda x: '(å­' not in x['äº§å“ç®€ç§°'], axis=1)]
        Baoyou_df = pd.concat([Baoyou_df_mom, Baoyou_df_sub])
        Baoyou_df.drop_duplicates(inplace=True)
        return Baoyou_df

    def load_channel_data(self) -> pd.DataFrame:
        """
        åŠ è½½æ¸ é“æ•°æ®

        è¿”å›ï¼š
            DataFrame: æ¸ é“æ•°æ®
        """
        return pd.read_excel(
            Config.get_weekly_data_file('å„æ¸ é“ä¿æœ‰é‡'),
            index_col=0
        )

    # ==================== 4. åº•å±‚æ•°æ®æ±‡æ€» ====================

    def load_classification_df(self) -> pd.DataFrame:
        """
        åŠ è½½å¤šèµ„äº§å“ç±»æ ‡ç­¾æ•°æ®

        è¿”å›ï¼š
            DataFrame: å“ç±»åˆ†ç±»æ•°æ®ï¼Œä»¥æ¯äº§å“ä»£ç ä¸ºç´¢å¼•
        """
        classification_df = pd.read_excel(
            Config.get_base_data_file('åº•å±‚æ•°æ®æ±‡æ€».xlsx'),
            sheet_name='å¤šèµ„äº§å“ç±»æ ‡ç­¾',
            index_col='æ¯äº§å“ä»£ç '
        )
        # å¤„ç†é‡å¤æ¯äº§å“ä»£ç 
        classification_df = classification_df[~classification_df.index.duplicated(keep='first')]
        return classification_df

    def load_parent_sort(self, columns: List[str] = None) -> pd.DataFrame:
        """
        åŠ è½½é”€å”®æˆ˜æŠ¥ç³»åˆ—åˆ†ç±»æ•°æ®

        å‚æ•°ï¼š
            columns: éœ€è¦è¿”å›çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤è¿”å›['æ¯åˆ†ç±»', 'æ€»åˆ†ç±»']

        è¿”å›ï¼š
            DataFrame: ç³»åˆ—åˆ†ç±»æ•°æ®ï¼Œä»¥å¤šèµ„äº§å“ç±»ä¸ºç´¢å¼•
        """
        if columns is None:
            columns = ['æ¯åˆ†ç±»', 'æ€»åˆ†ç±»']

        Parent_sort = pd.read_excel(
            Config.get_base_data_file('åº•å±‚æ•°æ®æ±‡æ€».xlsx'),
            sheet_name='é”€å”®æˆ˜æŠ¥ç³»åˆ—åˆ†ç±»',
            index_col=0
        )[columns]
        # å»é‡
        Parent_sort = Parent_sort[~Parent_sort.index.duplicated(keep='first')]
        return Parent_sort

    def load_asset_classification(self) -> pd.DataFrame:
        """
        åŠ è½½èµ„äº§åˆ†ç±»å‹¾ç¨½å…³ç³»æ•°æ®

        è¿”å›ï¼š
            DataFrame: èµ„äº§åˆ†ç±»æ•°æ®
        """
        return pd.read_excel(
            Config.get_base_data_file('åº•å±‚æ•°æ®æ±‡æ€».xlsx'),
            sheet_name='èµ„äº§åˆ†ç±»å‹¾ç¨½å…³ç³»',
            index_col=0
        )

    def load_public_account_tag(self) -> pd.DataFrame:
        """
        åŠ è½½å§”å¤–å…¬å…±ç­–ç•¥ä¸“æˆ·æ ‡ç­¾æ•°æ®

        è¿”å›ï¼š
            DataFrame: ä¸“æˆ·æ ‡ç­¾æ•°æ®
        """
        return pd.read_excel(
            Config.get_base_data_file('åº•å±‚æ•°æ®æ±‡æ€».xlsx'),
            sheet_name='å§”å¤–å…¬å…±ç­–ç•¥ä¸“æˆ·æ ‡ç­¾',
            index_col=0
        )

    def load_public_strategy(self) -> pd.DataFrame:
        """
        åŠ è½½å…¬å¸å…¬å…±ç­–ç•¥ä¸“æˆ·æ•°æ®

        è¿”å›ï¼š
            DataFrame: å…¬å…±ç­–ç•¥ä¸“æˆ·æ•°æ®
        """
        return pd.read_excel(
            Config.get_base_data_file('åº•å±‚æ•°æ®æ±‡æ€».xlsx'),
            sheet_name='å…¬å¸å…¬å…±ç­–ç•¥ä¸“æˆ·'
        )

    # ==================== 5. å†å²è¿ä½œæ¦‚è§ˆæ•°æ® ====================

    def load_overview_parent(self, date: str) -> pd.DataFrame:
        """
        åŠ è½½æŒ‡å®šæ—¥æœŸçš„æ¯äº§å“è¿ä½œæ¦‚è§ˆæ•°æ®

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)

        è¿”å›ï¼š
            DataFrame: æ¯äº§å“è¿ä½œæ¦‚è§ˆæ•°æ®
        """
        return pd.read_excel(
            Config.get_overview_parent_file(date),
            index_col=0
        )

    def load_overview_history(self, date: str) -> pd.DataFrame:
        """
        åŠ è½½æŒ‡å®šæ—¥æœŸçš„å†å²è¿ä½œæ¦‚è§ˆæ•°æ®ï¼ˆå«æ¯å­äº§å“ï¼‰

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)

        è¿”å›ï¼š
            DataFrame: å†å²è¿ä½œæ¦‚è§ˆæ•°æ®
        """
        return pd.read_excel(
            Config.get_overview_history_file(date),
            index_col=0
        )

    # ==================== 6. å‘¨åº¦æ›´æ–°æ•°æ® ====================

    def load_zhanbao_data(self) -> pd.DataFrame:
        """
        åŠ è½½æˆ˜æŠ¥åº•è¡¨æ•°æ®

        è¿”å›ï¼š
            DataFrame: æˆ˜æŠ¥æ•°æ®
        """
        zhanbao_df = pd.read_excel(
            Config.get_weekly_data_file('æˆ˜æŠ¥åº•è¡¨(v2)'),
            index_col=0
        )
        zhanbao_df = zhanbao_df[zhanbao_df['è§„æ¨¡ç±»å‹-ç®€åŒ–'] == 'é”€é‡']
        return zhanbao_df

    def load_chicang_data(self, date: str) -> pd.DataFrame:
        """
        åŠ è½½æŒä»“ç›ˆäºæ˜ç»†æ•°æ®

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)

        è¿”å›ï¼š
            DataFrame: æŒä»“æ˜ç»†æ•°æ®
        """
        excel_name = f'æŒä»“ç›ˆäºæ˜ç»†åˆ—è¡¨_{date}.xlsx'
        Chicang_df = pd.read_excel(
            os.path.join(Config.WEEKLY_DATA_DIR, excel_name)
        )
        Chicang_df.rename(columns={'èµ„äº§ç®€ç§°': 'å€ºåˆ¸ç®€ç§°'}, inplace=True)
        Chicang_df.drop_duplicates(inplace=True)
        return Chicang_df

    def load_zhongshou_data(self, date: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åŠ è½½ä¸­æ”¶åˆ†ææ•°æ®ï¼ˆæœˆåº¦å’Œå¹´åº¦ï¼‰

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)

        è¿”å›ï¼š
            Tuple[DataFrame, DataFrame]: (æœˆåº¦æ•°æ®, å¹´åº¦æ•°æ®)
        """
        zhongshou_m_path = os.path.join(
            Config.WEEKLY_DATA_DIR,
            f'ä¸­æ”¶åˆ†ææŠ¥è¡¨-äº§å“æ˜ç»†-{date}-m.xlsx'
        )
        zhongshou_y_path = os.path.join(
            Config.WEEKLY_DATA_DIR,
            f'ä¸­æ”¶åˆ†ææŠ¥è¡¨-äº§å“æ˜ç»†-{date}-y.xlsx'
        )
        zhongshou_m = pd.read_excel(zhongshou_m_path, index_col=0, skiprows=1)
        zhongshou_y = pd.read_excel(zhongshou_y_path, index_col=0, skiprows=1)
        return zhongshou_m, zhongshou_y

    def load_insurance_products(self) -> pd.DataFrame:
        """
        åŠ è½½ä¿é™©èµ„ç®¡è®¡åˆ’æ•°æ®

        è¿”å›ï¼š
            DataFrame: ä¿é™©èµ„ç®¡æ•°æ®
        """
        return pd.read_excel(
            Config.get_weekly_data_file('ä¿é™©èµ„ç®¡è®¡åˆ’'),
            index_col=0
        )

    def load_product_tosale(self) -> pd.DataFrame:
        """
        åŠ è½½ç†è´¢äº§å“åŒºé—´åœ¨å”®å¾…å”®æƒ…å†µè¡¨

        è¿”å›ï¼š
            DataFrame: å¾…å”®äº§å“æ•°æ®
        """
        return pd.read_excel(
            Config.get_weekly_data_file('ç†è´¢äº§å“åŒºé—´åœ¨å”®å¾…å”®æƒ…å†µè¡¨'),
            index_col=0,
            skiprows=1
        )

    def load_weiwai_overview(self) -> pd.DataFrame:
        """
        åŠ è½½å§”å¤–äº§å“æ¦‚è§ˆï¼ˆå›¢é˜Ÿè§†è§’ï¼‰

        è¿”å›ï¼š
            DataFrame: å§”å¤–äº§å“æ¦‚è§ˆæ•°æ®
        """
        return pd.read_excel(
            Config.get_weekly_data_file('å§”å¤–äº§å“æ¦‚è§ˆ(å›¢é˜Ÿè§†è§’)')
        )[['å§”å¤–èµ„äº§', 'æœ€æ–°è§„æ¨¡']]

    # ==================== 7. ä¸šç»©æ•°æ® ====================

    def load_performance_data(self, date: str) -> pd.DataFrame:
        """
        åŠ è½½äº§å“ä¸šç»©æ•°æ®

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)

        è¿”å›ï¼š
            DataFrame: ä¸šç»©æ•°æ®
        """
        return pd.read_excel(
            Config.get_performance_file(date),
            index_col=0
        )

    # ==================== 8. è¾“å‡ºç»“æœæ–‡ä»¶ ====================

    def load_output_result(self, date: str, sheet_name: str, **kwargs) -> pd.DataFrame:
        """
        åŠ è½½å†å²è¾“å‡ºç»“æœæ–‡ä»¶

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)
            sheet_name: å·¥ä½œè¡¨åç§°
            **kwargs: å…¶ä»–read_excelå‚æ•°

        è¿”å›ï¼š
            DataFrame: è¾“å‡ºç»“æœæ•°æ®
        """
        return pd.read_excel(
            Config.get_output_result_file(date),
            sheet_name=sheet_name,
            **kwargs
        )

    def load_asset_table(self, date: str) -> pd.DataFrame:
        """
        åŠ è½½èµ„äº§å¤§è¡¨æ•°æ®

        å‚æ•°ï¼š
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)

        è¿”å›ï¼š
            DataFrame: èµ„äº§å¤§è¡¨æ•°æ®
        """
        return pd.read_excel(
            Config.get_asset_table_file(date),
            index_col=0,
            header=[0, 1, 2]
        )


# ==================== è¾…åŠ©å‡½æ•° ====================

def map_product_xingtai(series_here: pd.Series) -> str:
    """
    åˆ¤æ–­äº§å“å½¢æ€

    å‚æ•°ï¼š
        series_here: äº§å“æ•°æ®è¡Œ

    è¿”å›ï¼š
        str: äº§å“å½¢æ€ (å°é—­/å®¢æˆ·å‘¨æœŸ/æ—¥å¼€/å®šå¼€)
    """
    if series_here['äº§å“å¼€æ”¾å½¢å¼'] == 'å°é—­å¼':
        return 'å°é—­'
    elif (series_here['äº§å“å¼€æ”¾å½¢å¼'] == 'å¼€æ”¾å¼') and (series_here['å‘¨æœŸå±æ€§'] == 'å®¢æˆ·å‘¨æœŸ'):
        return 'å®¢æˆ·å‘¨æœŸ'
    elif (series_here['äº§å“å¼€æ”¾å½¢å¼'] == 'å¼€æ”¾å¼') and (series_here['å‘¨æœŸå±æ€§'] == 'äº§å“å‘¨æœŸ') and (
            series_here['å‘¨æœŸå¤©æ•°'] == 1):
        return 'æ—¥å¼€'
    elif (series_here['äº§å“å¼€æ”¾å½¢å¼'] == 'å¼€æ”¾å¼') and (series_here['å‘¨æœŸå±æ€§'] == 'äº§å“å‘¨æœŸ'):
        return 'å®šå¼€'
    return None


def get_zhaoshang_label(set_here: set) -> str:
    """
    è·å–æ‹›å•†æ ‡ç­¾

    å‚æ•°ï¼š
        set_here: æ¸ é“åç§°é›†åˆ

    è¿”å›ï¼š
        str: æ‹›å•†æ ‡ç­¾ (æ‹›å•†ç‹¬æœ‰/æ‹›å•†å…±æœ‰/éæ‹›å•†)
    """
    if 'æ‹›å•†é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸' in set_here:
        if len(set_here.difference({'-', 'æ‹›å•†é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸'})) == 0:
            return 'æ‹›å•†ç‹¬æœ‰'
        else:
            return 'æ‹›å•†å…±æœ‰'
    else:
        return 'éæ‹›å•†'


def get_qudao_label_detail(set_here: set) -> str:
    """
    è·å–æ‰€æœ‰çš„æ¸ é“æ ‡ç­¾è¯¦æƒ…

    å‚æ•°ï¼š
        set_here: æ¸ é“åç§°é›†åˆ

    è¿”å›ï¼š
        str: æ¸ é“æ ‡ç­¾è¯¦æƒ…ï¼ˆé€—å·åˆ†éš”ï¼‰
    """
    qudao_forshort_dict = Config.CHANNEL_SHORT_NAMES
    qudao_rank_dict = dict(enumerate(qudao_forshort_dict.keys()))
    qudao_rank_dict = dict([val, key] for key, val in qudao_rank_dict.items())

    if "-" in set_here:
        set_here.remove("-")

    to_sort_list = list((x, qudao_rank_dict.get(x, 999)) for x in set_here)
    to_sort_list.sort(key=lambda y: y[1])

    return ",".join([qudao_forshort_dict.get(x[0], x[0]) for x in to_sort_list])


def del_Innovation_department(mid_df: pd.DataFrame) -> pd.DataFrame:
    """
    å»æ‰é‡åŒ–å’Œç»“æ„æ€§äº§å“ï¼Œå»æ‰åˆ›æ–°éƒ¨ç»ç†äº§å“

    å‚æ•°ï¼š
        mid_df: äº§å“æ•°æ®DataFrame

    è¿”å›ï¼š
        DataFrame: è¿‡æ»¤åçš„æ•°æ®
    """
    mid_df = mid_df[mid_df['å¤šèµ„äº§å“ç±»'].map(
        lambda x: x not in ['ç»“æ„æ€§ç†è´¢R2+', 'ç»“æ„æ€§ç†è´¢R3', 'ç»“æ„æ€§ç†è´¢R4', 'é‡åŒ–ç³»åˆ—', 'é‡åŒ–ç³»åˆ—(*)']
    )]
    mid_df = mid_df[mid_df['äº§å“ç»ç†'].map(
        lambda x: np.sum([k not in ['ç‹æµ©', 'å¼ é›…å©•', 'å­™æ–°å', ''] for k in str(x).split(',')]) != 0
    )]
    return mid_df


# ==================== ä¾¿æ·å‡½æ•° ====================

def get_data_loader() -> DataLoader:
    """è·å–æ•°æ®åŠ è½½å™¨å•ä¾‹"""
    if not hasattr(get_data_loader, '_instance'):
        get_data_loader._instance = DataLoader()
    return get_data_loader._instance


# ==================== æµ‹è¯•ä»£ç  ====================

if __name__ == '__main__':
    print("æ•°æ®åŠ è½½æ¨¡å—æµ‹è¯•")
    print("=" * 60)

    loader = DataLoader()

    print("\n1. æµ‹è¯•åŠ è½½DAPäº§å“ä¿¡æ¯...")
    product_info = loader.load_product_info_dap()
    print(f"   äº§å“æ•°é‡: {len(product_info)}")

    print("\n2. æµ‹è¯•åŠ è½½å“ç±»åˆ†ç±»...")
    classification = loader.load_classification_df()
    print(f"   å“ç±»æ•°é‡: {len(classification)}")

    print("\n3. æµ‹è¯•åŠ è½½ç³»åˆ—åˆ†ç±»...")
    parent_sort = loader.load_parent_sort()
    print(f"   åˆ†ç±»æ•°é‡: {len(parent_sort)}")

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
