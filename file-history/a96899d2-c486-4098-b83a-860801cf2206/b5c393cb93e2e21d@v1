#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡€å€¼æ•°æ®ç¼“å­˜ç®¡ç†å·¥å…·

ç”¨æ³•ï¼š
    python manage_nav_cache.py status   # æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
    python manage_nav_cache.py clear    # æ¸…é™¤ç¼“å­˜
    python manage_nav_cache.py rebuild  # å¼ºåˆ¶é‡å»ºç¼“å­˜
"""

import os
import sys
import pickle
import glob
from datetime import datetime
from config import Config

CACHE_DIR = os.path.join(Config.DATA_DIR, '.nav_cache')
CACHE_DATA_FILE = os.path.join(CACHE_DIR, 'nav_master_data.parquet')
CACHE_META_FILE = os.path.join(CACHE_DIR, 'nav_metadata.pkl')


def get_cache_status():
    """è·å–ç¼“å­˜çŠ¶æ€"""
    print("\n" + "=" * 60)
    print("ğŸ“Š å‡€å€¼æ•°æ®ç¼“å­˜çŠ¶æ€")
    print("=" * 60)

    print(f"\nç¼“å­˜ç›®å½•: {CACHE_DIR}")

    if not os.path.exists(CACHE_DIR):
        print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return

    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
    if os.path.exists(CACHE_DATA_FILE):
        size_mb = os.path.getsize(CACHE_DATA_FILE) / (1024 * 1024)
        mtime = datetime.fromtimestamp(os.path.getmtime(CACHE_DATA_FILE))
        print(f"\nç¼“å­˜æ•°æ®æ–‡ä»¶: {os.path.basename(CACHE_DATA_FILE)}")
        print(f"   å¤§å°: {size_mb:.2f} MB")
        print(f"   æ›´æ–°æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")

        # å°è¯•è¯»å–å¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        try:
            import pandas as pd
            df = pd.read_parquet(CACHE_DATA_FILE)
            print(f"   è®°å½•æ•°: {len(df):,} æ¡")
            if 'ä¼°å€¼æ—¥' in df.columns:
                dates = pd.to_datetime(df['ä¼°å€¼æ—¥'], errors='coerce')
                print(f"   æ—¥æœŸèŒƒå›´: {dates.min()} ~ {dates.max()}")
            if 'äº§å“ä»£ç ' in df.columns:
                print(f"   äº§å“æ•°é‡: {df['äº§å“ä»£ç '].nunique()} ä¸ª")
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•è¯»å–ç¼“å­˜æ•°æ®: {e}")
    else:
        print("\nâŒ ç¼“å­˜æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")

    # æ£€æŸ¥å…ƒæ•°æ®
    if os.path.exists(CACHE_META_FILE):
        try:
            with open(CACHE_META_FILE, 'rb') as f:
                meta = pickle.load(f)
            print(f"\nå…ƒæ•°æ®æ–‡ä»¶: {os.path.basename(CACHE_META_FILE)}")
            print(f"   å·²ç¼“å­˜æ–‡ä»¶æ•°: {len(meta)} ä¸ª")
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•è¯»å–å…ƒæ•°æ®: {e}")
    else:
        print("\nâŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")

    # æ£€æŸ¥æºæ–‡ä»¶å¤¹
    source_folder = Config.NET_VALUE_DIR
    excel_files = glob.glob(os.path.join(source_folder, '*.xlsx'))
    excel_files = [f for f in excel_files if not os.path.basename(f).startswith('~$')]
    print(f"\næºæ–‡ä»¶å¤¹: {source_folder}")
    print(f"   Excelæ–‡ä»¶æ•°: {len(excel_files)} ä¸ª")

    # è®¡ç®—éœ€è¦æ›´æ–°çš„æ–‡ä»¶
    if os.path.exists(CACHE_META_FILE):
        try:
            with open(CACHE_META_FILE, 'rb') as f:
                cached_meta = pickle.load(f)

            new_files = []
            modified_files = []
            for f in excel_files:
                mtime = os.path.getmtime(f)
                if f not in cached_meta:
                    new_files.append(os.path.basename(f))
                elif cached_meta[f] != mtime:
                    modified_files.append(os.path.basename(f))

            if new_files:
                print(f"\nğŸ“¥ æ–°å¢æ–‡ä»¶ ({len(new_files)} ä¸ª):")
                for f in new_files[:5]:
                    print(f"   - {f}")
                if len(new_files) > 5:
                    print(f"   ... ç­‰ {len(new_files) - 5} ä¸ªæ–‡ä»¶")

            if modified_files:
                print(f"\nğŸ“ å·²ä¿®æ”¹æ–‡ä»¶ ({len(modified_files)} ä¸ª):")
                for f in modified_files[:5]:
                    print(f"   - {f}")
                if len(modified_files) > 5:
                    print(f"   ... ç­‰ {len(modified_files) - 5} ä¸ªæ–‡ä»¶")

            if not new_files and not modified_files:
                print(f"\nâœ… ç¼“å­˜æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°")

        except Exception as e:
            print(f"\nâš ï¸ æ— æ³•æ£€æŸ¥æ›´æ–°çŠ¶æ€: {e}")

    print("\n" + "=" * 60)


def clear_cache():
    """æ¸…é™¤ç¼“å­˜"""
    print("\nğŸ—‘ï¸  æ¸…é™¤å‡€å€¼æ•°æ®ç¼“å­˜...")

    files_removed = 0
    for f in [CACHE_DATA_FILE, CACHE_META_FILE]:
        if os.path.exists(f):
            os.remove(f)
            print(f"   å·²åˆ é™¤: {os.path.basename(f)}")
            files_removed += 1

    if files_removed > 0:
        print(f"\nâœ… ç¼“å­˜å·²æ¸…é™¤ (åˆ é™¤äº† {files_removed} ä¸ªæ–‡ä»¶)")
    else:
        print("\nâš ï¸ ç¼“å­˜ç›®å½•ä¸ºç©ºï¼Œæ— éœ€æ¸…é™¤")


def rebuild_cache():
    """å¼ºåˆ¶é‡å»ºç¼“å­˜"""
    print("\nğŸ”„ å¼ºåˆ¶é‡å»ºå‡€å€¼æ•°æ®ç¼“å­˜...")

    # å…ˆæ¸…é™¤æ—§ç¼“å­˜
    clear_cache()

    # å¯¼å…¥å¹¶è°ƒç”¨ get_product_data
    print("\nå¼€å§‹é‡å»ºç¼“å­˜ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

    # åŠ¨æ€å¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "å‘¨æŠ¥ç»“æœè¡¨ç”Ÿæˆ",
        os.path.join(Config.BASE_DIR, "å‘¨æŠ¥ç»“æœè¡¨ç”Ÿæˆ.py")
    )

    # ç›´æ¥æ‰§è¡Œç¼“å­˜æ„å»º
    import pandas as pd
    import time

    source_folder = Config.NET_VALUE_DIR
    excel_files = glob.glob(os.path.join(source_folder, '*.xlsx'))
    excel_files = [f for f in excel_files if not os.path.basename(f).startswith('~$')]

    print(f"ğŸ“Š å‘ç° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")

    needed_cols = ['ä¼°å€¼æ—¥', 'äº§å“ä»£ç ', 'ç´¯è®¡å‡€å€¼']
    dfs = []
    start_time = time.time()

    for i, f in enumerate(excel_files, 1):
        if i % 10 == 0 or i == len(excel_files):
            print(f"   è¯»å–è¿›åº¦: {i}/{len(excel_files)}")
        try:
            df = pd.read_excel(f, usecols=needed_cols)
            df['_source_file'] = f
            dfs.append(df)
        except Exception as e:
            print(f"   âš ï¸ è¯»å–å¤±è´¥: {os.path.basename(f)} - {e}")

    read_time = time.time() - start_time
    print(f"   è¯»å–è€—æ—¶: {read_time:.1f} ç§’")

    if dfs:
        data = pd.concat(dfs, ignore_index=True)
        before = len(data)
        data.drop_duplicates(subset=['ä¼°å€¼æ—¥', 'äº§å“ä»£ç '], keep='last', inplace=True)
        print(f"   å»é‡: {before:,} -> {len(data):,} æ¡è®°å½•")

        # ä¿å­˜ç¼“å­˜
        os.makedirs(CACHE_DIR, exist_ok=True)
        data.to_parquet(CACHE_DATA_FILE, index=False)

        current_files_info = {f: os.path.getmtime(f) for f in excel_files}
        with open(CACHE_META_FILE, 'wb') as f:
            pickle.dump(current_files_info, f)

        print(f"\nâœ… ç¼“å­˜é‡å»ºå®Œæˆï¼")
        print(f"   ç¼“å­˜æ–‡ä»¶: {CACHE_DATA_FILE}")
        print(f"   è®°å½•æ•°: {len(data):,} æ¡")
    else:
        print("\nâŒ æ²¡æœ‰è¯»å–åˆ°ä»»ä½•æ•°æ®")


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        print("\nå½“å‰ç¼“å­˜çŠ¶æ€:")
        get_cache_status()
        return

    command = sys.argv[1].lower()

    if command == 'status':
        get_cache_status()
    elif command == 'clear':
        clear_cache()
    elif command == 'rebuild':
        rebuild_cache()
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
        print(__doc__)


if __name__ == "__main__":
    main()
